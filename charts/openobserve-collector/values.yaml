# Default values for openobserve-collector.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

exporters:
  otlphttp/openobserve:
    endpoint: https://api.openobserve.ai/api/default/
    headers:
      Authorization: Basic < base64 encoded auth >
  otlphttp/openobserve_k8s_events:
    endpoint: https://api.openobserve.ai/api/default/
    headers:
      Authorization: Basic < base64 encoded auth >
      stream-name: k8s_events

replicaCount: 1

image:
  repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.90.1"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

labels: {}

# If Opentelemetry operator should be installed with the chart. If you already have the operator installed, set enabled to false. Refer https://opentelemetry.io/docs/kubernetes/operator/ and https://opentelemetry.io/docs/kubernetes/helm/operator/
opentelemetry-operator:
  enabled: false
  admissionWebhooks:
    certManager:
      enabled: false
      autoGenerateCert: true
  manager:
    # auto-instrumentation can be enabled for go which is disabled by default. go auto-instrumentation uses eBPF and requires privileged mode.
    featureGates: "--operator.autoinstrumentation.go=true"

## Auto-Instrumentation resource to be installed in the cluster
## Can be used by setting the following to the pod/namespace annotations:
##  Java: instrumentation.opentelemetry.io/inject-java: "true"
##  NodeJS: instrumentation.opentelemetry.io/inject-nodejs: "true"
##  Python: instrumentation.opentelemetry.io/inject-python: "true"
##  DotNet: instrumentation.opentelemetry.io/inject-dotnet: "true"
##  Go: instrumentation.opentelemetry.io/inject-go: "true" , instrumentation.opentelemetry.io/otel-go-auto-target-exe: "/path/to/container/executable"
##  OpenTelemetry SDK environment variables only: instrumentation.opentelemetry.io/inject-sdk: "true"
autoinstrumentation:
  enabled: true
  ## The collector name to send traces to
  collectorTarget: traces
  propagators:
    - tracecontext
    - baggage

podAndServiceMonitor:
  # Specifies whether PodMonitor and ServiceMonitor CRs should be created
  create: true

podAnnotations: {}

podSecurityContext:
  {}
  # fsGroup: 2000

securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

agent:
  enabled: true
  tolerations: 
    - key: "exampleKey1"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  receivers:
    otlp:
      protocols:
        grpc: {}
        http: {}
    prometheus:
      config:
        scrape_configs:
          - job_name: "otel-collector"
            scrape_interval: 5s
            static_configs:
              - targets: ["0.0.0.0:8888"]
   
    filelog/std:
      include: [/var/log/pods/*/*/*.log]
      exclude:
        # Exclude logs from all containers named otel-collector or otc-container (otel-contrib)
        - /var/log/pods/*/otel-collector/*.log  # named otel-collector
        - /var/log/pods/*/otc-container/*.log   # named otc-container (for otel-contrib containers)
        - /var/log/pods/*/openobserve/*.log     # named openobserve. Would want to avoid cyclical logs
      start_at: end
      include_file_path: true
      include_file_name: false
      operators:
        # Find out which format is used by kubernetes
        - type: router
          id: get-format
          routes:
            - output: parser-docker
              expr: 'body matches "^\\{"'
            - output: parser-crio
              expr: 'body matches "^[^ Z]+ "'
            - output: parser-containerd
              expr: 'body matches "^[^ Z]+Z"'
        # Parse CRI-O format
        - type: regex_parser
          id: parser-crio
          regex: "^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout_type: gotime
            layout: "2006-01-02T15:04:05.999999999Z07:00"
        # Parse CRI-Containerd format
        - type: regex_parser
          id: parser-containerd
          regex: "^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: "%Y-%m-%dT%H:%M:%S.%LZ"
        # Parse Docker format
        - type: json_parser
          id: parser-docker
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: "%Y-%m-%dT%H:%M:%S.%LZ"
        # Extract metadata from file path
        - type: regex_parser
          id: extract_metadata_from_filepath
          regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
          parse_from: attributes["log.file.path"]
          cache:
            size: 128 # default maximum amount of Pods per Node is 110
        # Update body field after finishing all parsing
        - type: move
          from: attributes.log
          to: body
        # Rename attributes
        - type: move
          from: attributes.stream
          to: attributes["log.iostream"]
        - type: move
          from: attributes.container_name
          to: resource["k8s.container.name"]
        - type: move
          from: attributes.namespace
          to: resource["k8s.namespace.name"]
        - type: move
          from: attributes.pod_name
          to: resource["k8s.pod.name"]
        - type: move
          from: attributes.restart_count
          to: resource["k8s.container.restart_count"]
        - type: move
          from: attributes.uid
          to: resource["k8s.pod.uid"]
    # hostmetrics:
    #   root_path: /hostfs
    #   collection_interval: 15s
    #   scrapers:
    #     cpu: {}
    #     disk: {}
    #     filesystem:
    #       exclude_mount_points:
    #         match_type: regexp
    #         mount_points:
    #           - /dev/.*
    #           - /proc/.*
    #           - /sys/.*
    #           - /run/k3s/containerd/.*
    #           - /var/lib/docker/.*
    #           - /var/lib/kubelet/.*
    #           - /snap/.*
    #       exclude_fs_types:
    #         match_type: strict
    #         fs_types:
    #           - autofs
    #           - binfmt_misc
    #           - bpf
    #           - cgroup2
    #           - configfs
    #           - debugfs
    #           - devpts
    #           - devtmpfs
    #           - fusectl
    #           - hugetlbfs
    #           - iso9660
    #           - mqueue
    #           - nsfs
    #           - overlay
    #           - proc
    #           - procfs
    #           - pstore
    #           - rpc_pipefs
    #           - securityfs
    #           - selinuxfs
    #           - squashfs
    #           - sysfs
    #           - tracefs
    #     load: {}
    #     memory: {}
    #     network: {}
    #     paging: {}
    #     processes: {}
        # process: {} # a bug in the process scraper causes the collector to throw errors so disabling it for now
    kubeletstats:
      collection_interval: 15s
      auth_type: "serviceAccount"
      endpoint: "https://${env:K8S_NODE_NAME}:10250"
      insecure_skip_verify: true
      extra_metadata_labels:
        - container.id
        - k8s.volume.type
      metric_groups:
        - node
        - pod
        - container
        - volume
  processors:
    # resourcedetection:
    #   detectors: [system, env]
    #   override: true
    #   system:
    #     hostname_sources: [os, dns]
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      filter:
        node_from_env_var: K8S_NODE_NAME
      extract:
        labels:
          - tag_name: service.name
            key: app.kubernetes.io/name
            from: pod
          - tag_name: service.name
            key: k8s-app
            from: pod
          - tag_name: k8s.app.instance
            key: app.kubernetes.io/instance
            from: pod
          - tag_name: service.version
            key: app.kubernetes.io/version
            from: pod
          - tag_name: k8s.app.component
            key: app.kubernetes.io/component
            from: pod
        metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
            - from: resource_attribute
              name: k8s.node.name
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
        - sources:
            - from: connection
      # memory_limiter:
      #   check_interval: 1s
      #   limit_percentage: 75
      # spike_limit_percentage: 15
    batch:
      send_batch_size: 10000
      timeout: 10s
  extensions:
    zpages: {}
    # memory_ballast:
    #   # Memory Ballast size should be max 1/3 to 1/2 of memory. It's a large buffer that is pre-allocated in the memory to help stabilize and reduce the garbage collection (GC) pressure on the Go runtime.
    #   size_mib: 512
  connectors: {}
  service:
    extensions: [zpages]
    pipelines:
      logs:
        receivers: [filelog/std]
        processors: [batch, k8sattributes]
        exporters: [otlphttp/openobserve]
      metrics:
        receivers: [kubeletstats]
        processors: [batch, k8sattributes]
        exporters: [otlphttp/openobserve]
      traces:
        receivers: [otlp]
        processors: [batch, k8sattributes]
        exporters: [otlphttp/openobserve]

gateway:
  enabled: true
  affinity: {}
  nodeSelector: {}
  tolerations: []
  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
  receivers:
    otlp:
      protocols:
        grpc: {}
        http: {}
    k8s_cluster:
      collection_interval: 30s
      node_conditions_to_report:
        [Ready, MemoryPressure, DiskPressure, PIDPressure]
      allocatable_types_to_report: [cpu, memory, storage]
    k8s_events:
      auth_type: serviceAccount
    k8sobjects:
      auth_type: serviceAccount
      objects:
        - name: pods
          mode: pull
          # label_selector: environment in (production),tier in (frontend)
          field_selector: status.phase=Running
          interval: 15m
        - name: events
          mode: watch
          group: events.k8s.io
          # namespaces: []
    prometheus:
      config:
        scrape_configs:
          - job_name: "kubeApiServer"
            sample_limit: 10000
            # Default to scraping over https. If required, just disable this or change to `http`.
            scheme: http
            scrape_interval: 15s
            scrape_timeout: 10s
            metrics_path: /metrics

            kubernetes_sd_configs:
              - role: endpoints
                follow_redirects: true
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            metric_relabel_configs:
              - action: keep
                regex: default;kubernetes;https
                source_labels:
                - __meta_kubernetes_namespace
                - __meta_kubernetes_service_name
                - __meta_kubernetes_endpoint_port_name
              # Drop excessively noisy apiserver buckets.
              - action: drop
                regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
                source_labels:
                  - __name__
                  - le
          - job_name: "otel-collector"
            scrape_interval: 5s
            static_configs:
              - targets: ["0.0.0.0:8888"]
          - job_name: cadvisor
            scheme: https
            sample_limit: 10000
            scrape_interval: 15s
            scrape_timeout: 10s
            metrics_path: /metrics/cadvisor
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
            authorization:
              credentials_file: "/var/run/secrets/kubernetes.io/serviceaccount/token"
              type: Bearer
            kubernetes_sd_configs:
              - role: node
            # static_configs:
            #   - targets:
            #       - ${K8S_NODE_NAME}:10250
            metric_relabel_configs:
              - action: labeldrop
                regex: name # dropping id results in error - inconsistent timestamps on metric points for metric container_fs_reads_total, container_fs_writes_bytes_total, etc
              # Drop less useful container CPU metrics.
              - action: drop
                regex: container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)
                replacement: "$1"
                separator: ";"
                source_labels:
                  - __name__
              # Drop less useful container / always zero filesystem metrics.
              - action: drop
                regex: container_fs_(io_current|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)
                separator: ";"
                source_labels:
                  - __name__
              # Drop less useful / always zero container memory metrics.
              - action: drop
                regex: container_memory_(mapped_file|swap)
                replacement: "$1"
                separator: ";"
                source_labels:
                  - __name__
              # Drop less useful container process metrics.
              - action: drop
                regex: container_(file_descriptors|tasks_state|threads_max)
                replacement: "$1"
                separator: ";"
                source_labels:
                  - __name__
              # Drop container spec metrics that overlap with kube-state-metrics.
              - action: drop
                regex: container_spec.*
                replacement: "$1"
                separator: ";"
                source_labels:
                  - __name__
              # Drop cgroup metrics with no pod.
              - action: drop
                regex: ".+;"
                replacement: "$1"
                separator: ";"
                source_labels:
                  - id
                  - pod
            relabel_configs:
              - action: replace
                regex: "(.*)"
                replacement: https-metrics
                separator: ";"
                target_label: endpoint
              - action: replace
                replacement: "kubelet"
                target_label: job
              - action: replace
                regex: "(.*)"
                replacement: "${1}"
                separator: ";"
                source_labels:
                  - __meta_kubernetes_node_name
                target_label: node
              - action: replace
                regex: "(.*)"
                replacement: "$1"
                separator: ";"
                source_labels:
                  - __metrics_path__
                target_label: metrics_path

  processors:
    resourcedetection:
      detectors: [env]
      override: true
      timeout: 2s
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      # filter:
      #   node_from_env_var: K8S_NODE_NAME
      extract:
        labels:
          - tag_name: service.name
            key: app.kubernetes.io/name
            from: pod
          - tag_name: service.name
            key: k8s-app
            from: pod
          - tag_name: k8s.app.instance
            key: app.kubernetes.io/instance
            from: pod
          - tag_name: service.version
            key: app.kubernetes.io/version
            from: pod
          - tag_name: k8s.app.component
            key: app.kubernetes.io/component
            from: pod
        metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.node.name
          - k8s.pod.start_time
          - k8s.deployment.name
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.container.name
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.tag
          - container.image.name
          - k8s.cluster.uid
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
            - from: resource_attribute
              name: k8s.node.name
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
        - sources:
            - from: connection
    # memory_limiter:
    #   check_interval: 1s
    #   limit_percentage: 75
    #   spike_limit_percentage: 15
    batch:
      send_batch_size: 10000
      timeout: 10s
  extensions:
    zpages: {}
    # memory_ballast:
    #   # Memory Ballast size should be max 1/3 to 1/2 of memory. It's a large buffer that is pre-allocated in the memory to help stabilize and reduce the garbage collection (GC) pressure on the Go runtime.
    #   size_mib: 512
  connectors:
    spanmetrics:
      histogram:
        explicit:
          buckets:
            [
              100us,
              1ms,
              2ms,
              6ms,
              10ms,
              100ms,
              250ms,
              500ms,
              1000ms,
              1400ms,
              2000ms,
              5s,
              10s,
              30s,
              60s,
              120s,
              300s,
              600s,
            ]
      dimensions:
        - name: http.method
          default: GET
        - name: http.status_code
      exemplars:
        enabled: true
      dimensions_cache_size: 1000
      aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
      metrics_flush_interval: 15s
    servicegraph:
      latency_histogram_buckets: [1, 2, 3, 4, 5]
      dimensions:
        - http.method
      store:
        ttl: 1s
        max_items: 10
  service:
    extensions: [zpages]
    pipelines:
      logs/k8s_events:
        receivers: [k8s_events]
        processors: [batch, k8sattributes, resourcedetection]
        exporters: [otlphttp/openobserve_k8s_events]
      metrics:
        receivers: [k8s_cluster, prometheus, spanmetrics, servicegraph]
        processors: [batch, k8sattributes, resourcedetection]
        exporters: [otlphttp/openobserve]
      traces:
        receivers: [otlp]
        processors: [batch, k8sattributes, resourcedetection]
        exporters: [otlphttp/openobserve, spanmetrics, servicegraph]
