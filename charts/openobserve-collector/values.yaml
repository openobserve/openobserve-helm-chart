# Default values for openobserve-collector.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

exporters:
  otlphttp/openobserve:
    endpoint: https://api.openobserve.ai/api/default/
    headers:
      Authorization: Basic YOUR_BASE64_ENCODED_AUTH
  otlphttp/openobserve_k8s_events:
    endpoint: https://api.openobserve.ai/api/default/
    headers:
      Authorization: Basic CHANGEME_BASE64_ENCODED_AUTH
      stream-name: k8s_events

k8sCluster: "cluster1"

image:
  repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.138.0"

nameOverride: ""
fullnameOverride: ""
clusterDomain: "cluster.local"

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# If Opentelemetry operator should be installed with the chart. If you already have the operator installed, set enabled to false. Refer https://opentelemetry.io/docs/kubernetes/operator/ and https://opentelemetry.io/docs/kubernetes/helm/operator/
opentelemetry-operator:
  enabled: false
  admissionWebhooks:
    certManager:
      enabled: false
      autoGenerateCert: true
  manager:
    # auto-instrumentation can be enabled for go which is disabled by default. go auto-instrumentation uses eBPF and requires privileged mode.
    featureGates: "--operator.autoinstrumentation.go=true"

## Auto-Instrumentation resource to be installed in the cluster
## Can be used by setting the following to the pod/namespace annotations:
##  Java: instrumentation.opentelemetry.io/inject-java: "true"
##  NodeJS: instrumentation.opentelemetry.io/inject-nodejs: "true"
##  Python: instrumentation.opentelemetry.io/inject-python: "true"
##  DotNet: instrumentation.opentelemetry.io/inject-dotnet: "true"
##  Go: instrumentation.opentelemetry.io/inject-go: "true" , instrumentation.opentelemetry.io/otel-go-auto-target-exe: "/path/to/container/executable"
##  OpenTelemetry SDK environment variables only: instrumentation.opentelemetry.io/inject-sdk: "true"
autoinstrumentation:
  enabled: true
  # Sampling rate for traces (0.0 to 1.0). 0.1 = 10%, 1.0 = 100%
  samplingRate: "1.0"
  # Go instrumentation image version
  goImageVersion: "v0.23.0"
  # Enable Python logs auto-instrumentation
  pythonLogsEnabled: false

agent:
  enabled: true
  tolerations: []
  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  receivers:
    filelog/std:
      include: [/var/log/pods/*/*/*.log]
      exclude:
        # Exclude logs from all containers named otel-collector or otc-container (otel-contrib)
        - /var/log/pods/*/otel-collector/*.log # named otel-collector
        - /var/log/pods/*/otc-container/*.log # named otc-container (for otel-contrib containers)
        - /var/log/pods/*/openobserve-ingester/*.log # avoid cyclical logs as ingester logs can be massive
      start_at: beginning
      include_file_path: true
      include_file_name: false
      operators:
        # Find out which format is used by kubernetes
        - type: router
          id: get-format
          routes:
            - output: parser-docker
              expr: 'body matches "^\\{"'
            - output: parser-crio
              expr: 'body matches "^[^ Z]+ "'
            - output: parser-containerd
              expr: 'body matches "^[^ Z]+Z"'
        # Parse CRI-O format
        - type: regex_parser
          id: parser-crio
          regex: "^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout_type: gotime
            layout: "2006-01-02T15:04:05.999999999Z07:00"
        # Parse CRI-Containerd format
        - type: regex_parser
          id: parser-containerd
          regex: "^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: "%Y-%m-%dT%H:%M:%S.%LZ"
        # Parse Docker format
        - type: json_parser
          id: parser-docker
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: "%Y-%m-%dT%H:%M:%S.%LZ"
        # Extract pod UID from file path for k8sattributes processor association
        - type: regex_parser
          id: extract_metadata_from_filepath
          regex: '^.*\/[^_]+_[^_]+_(?P<uid>[a-f0-9\-]{36})\/'
          parse_from: attributes["log.file.path"]
          cache:
            size: 128
        # Update body field after finishing all parsing
        - type: move
          from: attributes.log
          to: body
        # Rename stream attribute
        - type: move
          from: attributes.stream
          to: attributes["log.iostream"]
        # Move pod UID to resource for k8sattributes processor association
        - type: move
          from: attributes.uid
          to: resource["k8s.pod.uid"]
    hostmetrics:
      root_path: /hostfs
      collection_interval: 30s
      scrapers:
        cpu:
          metrics:
            system.cpu.utilization:
              enabled: true
        disk: {}
        filesystem:
          exclude_mount_points:
            match_type: regexp
            mount_points:
              - /dev/.*
              - /proc/.*
              - /sys/.*
              - /run/k3s/containerd/.*
              - /var/lib/docker/.*
              - /var/lib/kubelet/.*
              - /snap/.*
          exclude_fs_types:
            match_type: strict
            fs_types:
              - autofs
              - binfmt_misc
              - bpf
              - cgroup2
              - configfs
              - debugfs
              - devpts
              - devtmpfs
              - fusectl
              - hugetlbfs
              - iso9660
              - mqueue
              - nsfs
              - overlay
              - proc
              - procfs
              - pstore
              - rpc_pipefs
              - securityfs
              - selinuxfs
              - squashfs
              - sysfs
              - tracefs
        load: {}
        memory:
          metrics:
            system.memory.utilization:
              enabled: true
        network: {}
        #     paging: {}
        #     processes: {}
        #     process: {} # a bug in the process scraper causes the collector to throw errors so disabling it for now
    kubeletstats:
      collection_interval: 30s
      auth_type: "serviceAccount"
      endpoint: "https://${env:K8S_NODE_IP}:10250"
      insecure_skip_verify: true
      extra_metadata_labels:
        - container.id
        - k8s.volume.type
      metric_groups:
        - node
        - pod
        - container
        - volume
      metrics:
        # CPU usage metrics (default since v0.125.0+, represents raw CPU usage in cores)
        k8s.pod.cpu.usage:
          enabled: true
        k8s.node.cpu.usage:
          enabled: true
        k8s.node.cpu.time:
          enabled: true
        container.cpu.usage:
          enabled: true
        # Utilization metrics (percentage-based, calculated against limits/requests)
        k8s.pod.cpu_limit_utilization:
          enabled: true
        k8s.pod.cpu_request_utilization:
          enabled: true
        k8s.pod.memory_limit_utilization:
          enabled: true
        k8s.pod.memory_request_utilization:
          enabled: true
  processors:
    memory_limiter:
      check_interval: 1s
      limit_mib: 512
      spike_limit_mib: 128
    resourcedetection:
      detectors: [system, env, k8snode]
      override: true
      system:
        hostname_sources: [os, dns]
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      filter:
        node_from_env_var: K8S_NODE_NAME
      extract:
        labels:
          - tag_name: service.name
            key: app.kubernetes.io/name
            from: pod
          - tag_name: k8s.app.name
            key: k8s-app
            from: pod
          - tag_name: k8s.app.instance
            key: app.kubernetes.io/instance
            from: pod
          - tag_name: service.version
            key: app.kubernetes.io/version
            from: pod
          - tag_name: k8s.app.component
            key: app.kubernetes.io/component
            from: pod
        metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
          - k8s.container.name
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
            - from: resource_attribute
              name: k8s.node.name
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
        - sources:
            - from: connection
    batch:
      send_batch_size: 2000
      send_batch_max_size: 5000
      timeout: 5s
  extensions:
    health_check:
      endpoint: 0.0.0.0:13133
    zpages: {}
    # memory_ballast:
    #   # Memory Ballast size should be max 1/3 to 1/2 of memory. It's a large buffer that is pre-allocated in the memory to help stabilize and reduce the garbage collection (GC) pressure on the Go runtime.
    #   size_mib: 512
  connectors: {}
  service:
    extensions: [health_check, zpages]
    pipelines:
      logs:
        receivers: [filelog/std]
        processors:
          [memory_limiter, resourcedetection, attributes, k8sattributes, batch]
        exporters: [otlphttp/openobserve] 
      metrics:
        receivers: [kubeletstats, hostmetrics]
        processors:
          [memory_limiter, resourcedetection, attributes, k8sattributes, batch]
        exporters: [otlphttp/openobserve] 

gateway:
  enabled: true
  # Number of replicas when autoscaler is not used
  replicas: 1
  targetAllocator:
    enabled: false
    resources: {}
      # We usually recommend not to specify default resources and to leave this as a conscious
      # choice for the user. This also increases chances charts run on environments with little
      # resources, such as Minikube. If you do want to specify resources, uncomment the following
      # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
      # requests:
      #   cpu: 50m
      #   memory: 64Mi
  affinity: {}
  nodeSelector: {}
  tolerations: []
  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  # Autoscaler configuration for the OpenTelemetry Operator
  # When enabled (by setting any field), the operator will create a HorizontalPodAutoscaler
  # Note: Autoscaling requires resources.requests to be set for CPU/memory metrics to work
  autoscaler: {}
    # minReplicas: 1
    # maxReplicas: 10
    # targetCPUUtilization: 80
    # targetMemoryUtilization: 80
    # # Advanced: Custom HPA behavior (optional)
    # behavior:
    #   scaleDown:
    #     stabilizationWindowSeconds: 300
    #     policies:
    #     - type: Percent
    #       value: 50
    #       periodSeconds: 15
    #   scaleUp:
    #     stabilizationWindowSeconds: 0
    #     policies:
    #     - type: Percent
    #       value: 100
    #       periodSeconds: 15
    # # Advanced: Custom metrics (optional)
    # metrics: []
  receivers:
    otlp:
      protocols:
        grpc: {}
        http: {}
    k8s_cluster:
      collection_interval: 30s
      node_conditions_to_report:
        [Ready, MemoryPressure, DiskPressure, PIDPressure]
      allocatable_types_to_report: [cpu, memory, storage]
      metrics:
        k8s.container.cpu_limit: # redundant
          enabled: false
        k8s.container.cpu_request: # redundant
          enabled: false
        k8s.container.memory_limit: # redundant
          enabled: false
        k8s.container.memory_request: # redundant
          enabled: false
    k8sobjects/events:
      auth_type: serviceAccount
      objects:
        - name: events
          mode: watch
          group: events.k8s.io
          # namespaces: []
    k8sobjects/pods:
      auth_type: serviceAccount
      objects:
        - name: pods
          mode: watch
          # label_selector: environment in (production),tier in (frontend)
          # field_selector filters removed to capture all pod phases (Pending, Running, Failed, etc.)
    prometheus:
      config:
        scrape_configs:
          - job_name: cadvisor
            scheme: https
            sample_limit: 10000
            scrape_interval: 30s
            scrape_timeout: 10s
            metrics_path: /metrics/cadvisor
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
            authorization:
              credentials_file: "/var/run/secrets/kubernetes.io/serviceaccount/token"
              type: Bearer
            kubernetes_sd_configs:
              - role: node
            metric_relabel_configs:
              # Keep container filesystem read/write metrics for IOPS
              - action: keep
                regex: container_fs_(reads_total|writes_total|reads_bytes_total|writes_bytes_total)
                source_labels:
                  - __name__
              # Drop cgroup metrics with no pod
              - action: drop
                regex: ".+;"
                separator: ";"
                source_labels:
                  - id
                  - pod
            relabel_configs:
              - action: replace
                replacement: "kubelet"
                target_label: job
              - action: replace
                source_labels:
                  - __meta_kubernetes_node_name
                target_label: node

  processors:
    memory_limiter:
      check_interval: 1s
      limit_mib: 1024
      spike_limit_mib: 256
    resourcedetection:
      detectors: [env]
      override: true
      timeout: 2s
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      # filter:
      #   node_from_env_var: K8S_NODE_NAME
      extract:
        labels:
          - tag_name: service.name
            key: app.kubernetes.io/name
            from: pod
          - tag_name: k8s.app.name
            key: k8s-app
            from: pod
          - tag_name: k8s.app.instance
            key: app.kubernetes.io/instance
            from: pod
          - tag_name: service.version
            key: app.kubernetes.io/version
            from: pod
          - tag_name: k8s.app.component
            key: app.kubernetes.io/component
            from: pod
        metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.node.name
          - k8s.pod.start_time
          - k8s.deployment.name
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.container.name
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.tag
          - container.image.name
          - k8s.cluster.uid
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
            - from: resource_attribute
              name: k8s.node.name
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
        - sources:
            - from: connection
    batch:
      send_batch_size: 2000
      send_batch_max_size: 5000
      timeout: 5s
  extensions:
    health_check:
      endpoint: 0.0.0.0:13133
    zpages: {}
    # memory_ballast:
    #   # Memory Ballast size should be max 1/3 to 1/2 of memory. It's a large buffer that is pre-allocated in the memory to help stabilize and reduce the garbage collection (GC) pressure on the Go runtime.
    #   size_mib: 512
  connectors:
    servicegraph:
      latency_histogram_buckets:
        [
          2ms,
          4ms,
          6ms,
          8ms,
          10ms,
          50ms,
          100ms,
          200ms,
          400ms,
          800ms,
          1s,
          1400ms,
          2s,
          5s,
          10s,
          15s,
        ]
      dimensions:
        - http.method
      store:
        ttl: 30s
        max_items: 10000
  service:
    extensions: [health_check, zpages]
    pipelines:
      logs/k8s_events:
        receivers: [k8sobjects/events]
        processors:
          [memory_limiter, resourcedetection, attributes, k8sattributes, batch]
        exporters: [otlphttp/openobserve_k8s_events]
      logs/k8s_pods:
        receivers: [k8sobjects/pods]
        processors:
          [memory_limiter, resourcedetection, attributes, k8sattributes, batch]
        exporters: [otlphttp/openobserve] 
      metrics:
        receivers: [k8s_cluster, servicegraph, prometheus]
        processors:
          [memory_limiter, resourcedetection, attributes, k8sattributes, batch]
        exporters: [otlphttp/openobserve] 
      traces:
        receivers: [otlp]
        processors:
          [memory_limiter, resourcedetection, attributes, k8sattributes, batch]
        exporters: [otlphttp/openobserve, servicegraph] 