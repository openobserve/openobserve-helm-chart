# Default values for openobserve-collector.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

exporters:
  otlphttp/openobserve:
    endpoint: https://api.openobserve.ai/api/default/
    headers:
      Authorization: Basic < base64 encoded auth >
  otlphttp/openobserve_k8s_events:
    endpoint: https://api.openobserve.ai/api/default/
    headers:
      Authorization: Basic < base64 encoded auth >
      stream-name: k8s_events

replicaCount: 1

image:
  repository: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "0.99.0"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

labels: {}

# If Opentelemetry operator should be installed with the chart. If you already have the operator installed, set enabled to false. Refer https://opentelemetry.io/docs/kubernetes/operator/ and https://opentelemetry.io/docs/kubernetes/helm/operator/
opentelemetry-operator:
  enabled: false
  admissionWebhooks:
    certManager:
      enabled: false
      autoGenerateCert: true
  manager:
    # auto-instrumentation can be enabled for go which is disabled by default. go auto-instrumentation uses eBPF and requires privileged mode.
    featureGates: "--operator.autoinstrumentation.go=true"

## Auto-Instrumentation resource to be installed in the cluster
## Can be used by setting the following to the pod/namespace annotations:
##  Java: instrumentation.opentelemetry.io/inject-java: "true"
##  NodeJS: instrumentation.opentelemetry.io/inject-nodejs: "true"
##  Python: instrumentation.opentelemetry.io/inject-python: "true"
##  DotNet: instrumentation.opentelemetry.io/inject-dotnet: "true"
##  Go: instrumentation.opentelemetry.io/inject-go: "true" , instrumentation.opentelemetry.io/otel-go-auto-target-exe: "/path/to/container/executable"
##  OpenTelemetry SDK environment variables only: instrumentation.opentelemetry.io/inject-sdk: "true"
autoinstrumentation:
  enabled: true
  ## The collector name to send traces to
  collectorTarget: traces
  propagators:
    - tracecontext
    - baggage

podAndServiceMonitor:
  # Specifies whether PodMonitor and ServiceMonitor CRs should be created
  create: true

podAnnotations: {}

podSecurityContext:
  {}
  # fsGroup: 2000

securityContext:
  {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

agent:
  enabled: true
  tolerations: 
    - key: "exampleKey1"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  receivers:
    otlp:
      protocols:
        grpc: {}
        http: {}
    prometheus:
      config:
        scrape_configs:
          - job_name: "otel-collector"
            scrape_interval: 5s
            static_configs:
              - targets: ["0.0.0.0:8888"]
   
    filelog/std:
      include: [/var/log/pods/*/*/*.log]
      exclude:
        # Exclude logs from all containers named otel-collector or otc-container (otel-contrib)
        - /var/log/pods/*/otel-collector/*.log  # named otel-collector
        - /var/log/pods/*/otc-container/*.log   # named otc-container (for otel-contrib containers)
        - /var/log/pods/*/openobserve/*.log     # named openobserve. Would want to avoid cyclical logs
      start_at: end
      include_file_path: true
      include_file_name: false
      operators:
        # Find out which format is used by kubernetes
        - type: router
          id: get-format
          routes:
            - output: parser-docker
              expr: 'body matches "^\\{"'
            - output: parser-crio
              expr: 'body matches "^[^ Z]+ "'
            - output: parser-containerd
              expr: 'body matches "^[^ Z]+Z"'
        # Parse CRI-O format
        - type: regex_parser
          id: parser-crio
          regex: "^(?P<time>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout_type: gotime
            layout: "2006-01-02T15:04:05.999999999Z07:00"
        # Parse CRI-Containerd format
        - type: regex_parser
          id: parser-containerd
          regex: "^(?P<time>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<log>.*)$"
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: "%Y-%m-%dT%H:%M:%S.%LZ"
        # Parse Docker format
        - type: json_parser
          id: parser-docker
          output: extract_metadata_from_filepath
          timestamp:
            parse_from: attributes.time
            layout: "%Y-%m-%dT%H:%M:%S.%LZ"
        # Extract metadata from file path
        - type: regex_parser
          id: extract_metadata_from_filepath
          regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
          parse_from: attributes["log.file.path"]
          cache:
            size: 128 # default maximum amount of Pods per Node is 110
        # Update body field after finishing all parsing
        - type: move
          from: attributes.log
          to: body
        # Rename attributes
        - type: move
          from: attributes.stream
          to: attributes["log.iostream"]
        - type: move
          from: attributes.container_name
          to: resource["k8s.container.name"]
        - type: move
          from: attributes.namespace
          to: resource["k8s.namespace.name"]
        - type: move
          from: attributes.pod_name
          to: resource["k8s.pod.name"]
        - type: move
          from: attributes.restart_count
          to: resource["k8s.container.restart_count"]
        - type: move
          from: attributes.uid
          to: resource["k8s.pod.uid"]
    hostmetrics:
      root_path: /hostfs
      collection_interval: 15s
      scrapers:
        cpu: {}
        disk: {}
        filesystem:
          exclude_mount_points:
            match_type: regexp
            mount_points:
              - /dev/.*
              - /proc/.*
              - /sys/.*
              - /run/k3s/containerd/.*
              - /var/lib/docker/.*
              - /var/lib/kubelet/.*
              - /snap/.*
          exclude_fs_types:
            match_type: strict
            fs_types:
              - autofs
              - binfmt_misc
              - bpf
              - cgroup2
              - configfs
              - debugfs
              - devpts
              - devtmpfs
              - fusectl
              - hugetlbfs
              - iso9660
              - mqueue
              - nsfs
              - overlay
              - proc
              - procfs
              - pstore
              - rpc_pipefs
              - securityfs
              - selinuxfs
              - squashfs
              - sysfs
              - tracefs
        load: {}
    #     memory: {}
        network: {}
    #     paging: {}
    #     processes: {}
        process: {} # a bug in the process scraper causes the collector to throw errors so disabling it for now
    kubeletstats:
      collection_interval: 15s
      auth_type: "serviceAccount"
      endpoint: "https://${env:K8S_NODE_NAME}:10250"
      insecure_skip_verify: true
      extra_metadata_labels:
        - container.id
        - k8s.volume.type
      metric_groups:
        - node
        - pod
        - container
        - volume
      metrics:
        k8s.pod.cpu_limit_utilization:
          enabled: true
        k8s.pod.cpu_request_utilization:
          enabled: true
        k8s.pod.memory_limit_utilization:
          enabled: true
        k8s.pod.memory_request_utilization:
          enabled: true
  processors:
    resourcedetection:
      detectors: [system, env, k8snode]
      override: true
      system:
        hostname_sources: [os, dns]
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      filter:
        node_from_env_var: K8S_NODE_NAME
      extract:
        labels:
          - tag_name: service.name
            key: app.kubernetes.io/name
            from: pod
          - tag_name: service.name
            key: k8s-app
            from: pod
          - tag_name: k8s.app.instance
            key: app.kubernetes.io/instance
            from: pod
          - tag_name: service.version
            key: app.kubernetes.io/version
            from: pod
          - tag_name: k8s.app.component
            key: app.kubernetes.io/component
            from: pod
        metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.pod.start_time
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
            - from: resource_attribute
              name: k8s.node.name
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
        - sources:
            - from: connection
      # memory_limiter:
      #   check_interval: 1s
      #   limit_percentage: 75
      # spike_limit_percentage: 15
    batch:
      send_batch_size: 10000
      timeout: 10s
  extensions:
    zpages: {}
    # memory_ballast:
    #   # Memory Ballast size should be max 1/3 to 1/2 of memory. It's a large buffer that is pre-allocated in the memory to help stabilize and reduce the garbage collection (GC) pressure on the Go runtime.
    #   size_mib: 512
  connectors: {}
  service:
    extensions: [zpages]
    pipelines:
      logs:
        receivers: [filelog/std]
        processors: [batch, k8sattributes]
        exporters: [otlphttp/openobserve]
      metrics:
        receivers: [kubeletstats]
        processors: [batch, k8sattributes]
        exporters: [otlphttp/openobserve]
      traces:
        receivers: [otlp]
        processors: [batch, k8sattributes]
        exporters: [otlphttp/openobserve]

gateway:
  enabled: true
  targetAllocator:
    enabled: true
  affinity: {}
  nodeSelector: {}
  tolerations: []
  resources:
    {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80
  receivers:
    otlp:
      protocols:
        grpc: {}
        http: {}
    k8s_cluster:
      collection_interval: 30s
      node_conditions_to_report:
        [Ready, MemoryPressure, DiskPressure, PIDPressure]
      allocatable_types_to_report: [cpu, memory, storage]
      metrics:
        k8s.container.cpu_limit: # redundant
          enabled: false
        k8s.container.cpu_request:  # redundant
          enabled: false
        k8s.container.memory_limit: # redundant
          enabled: false
        k8s.container.memory_request: # redundant
          enabled: false
    k8s_events:
      auth_type: serviceAccount
    k8sobjects:
      auth_type: serviceAccount
      objects:
        - name: pods
          mode: pull
          # label_selector: environment in (production),tier in (frontend)
          field_selector: status.phase=Running
          interval: 15m
        - name: events
          mode: watch
          group: events.k8s.io
          # namespaces: []
    # Most folks don't need prometheus receiver since default dashboards get data from kubeletstats receiver, so it's commented out by default
    prometheus:
      config:
        scrape_configs:
          # - job_name: "kubeApiServer"
          #   sample_limit: 10000
          #   # Default to scraping over https. If required, just disable this or change to `http`.
          #   scheme: http
          #   scrape_interval: 15s
          #   scrape_timeout: 10s
          #   metrics_path: /metrics

          #   kubernetes_sd_configs:
          #     - role: endpoints
          #       follow_redirects: true
          #   tls_config:
          #     ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          #     insecure_skip_verify: true
          #   bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

          #   metric_relabel_configs:
          #     - action: keep
          #       regex: default;kubernetes;https
          #       source_labels:
          #       - __meta_kubernetes_namespace
          #       - __meta_kubernetes_service_name
          #       - __meta_kubernetes_endpoint_port_name
          #     # Drop excessively noisy apiserver buckets.
          #     - action: drop
          #       regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)
          #       source_labels:
          #         - __name__
          #         - le
          # - job_name: "otel-collector"
          #   scrape_interval: 5s
          #   static_configs:
          #     - targets: ["0.0.0.0:8888"]
          # - job_name: cadvisor
          #   scheme: https
          #   sample_limit: 10000
          #   scrape_interval: 15s
          #   scrape_timeout: 10s
          #   metrics_path: /metrics/cadvisor
          #   tls_config:
          #     ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          #     insecure_skip_verify: true
          #   authorization:
          #     credentials_file: "/var/run/secrets/kubernetes.io/serviceaccount/token"
          #     type: Bearer
          #   kubernetes_sd_configs:
              # - role: node
            # static_configs:
            #   - targets:
            #       - ${K8S_NODE_NAME}:10250
            # metric_relabel_configs:
            #   - action: labeldrop
            #     regex: name # dropping id results in error - inconsistent timestamps on metric points for metric container_fs_reads_total, container_fs_writes_bytes_total, etc
            #   # Drop less useful container CPU metrics.
            #   - action: drop
            #     regex: container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)
            #     replacement: "$1"
            #     separator: ";"
            #     source_labels:
            #       - __name__
            #   # Drop less useful container / always zero filesystem metrics.
            #   - action: drop
            #     regex: container_fs_(io_current|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)
            #     separator: ";"
            #     source_labels:
            #       - __name__
            #   # Drop less useful / always zero container memory metrics.
            #   - action: drop
            #     regex: container_memory_(mapped_file|swap)
            #     replacement: "$1"
            #     separator: ";"
            #     source_labels:
            #       - __name__
            #   # Drop less useful container process metrics.
            #   - action: drop
            #     regex: container_(file_descriptors|tasks_state|threads_max)
            #     replacement: "$1"
            #     separator: ";"
            #     source_labels:
            #       - __name__
            #   # Drop container spec metrics that overlap with kube-state-metrics.
            #   - action: drop
            #     regex: container_spec.*
            #     replacement: "$1"
            #     separator: ";"
            #     source_labels:
            #       - __name__
            #   # Drop cgroup metrics with no pod.
            #   - action: drop
            #     regex: ".+;"
            #     replacement: "$1"
            #     separator: ";"
            #     source_labels:
            #       - id
            #       - pod
            # relabel_configs:
            #   - action: replace
            #     regex: "(.*)"
            #     replacement: https-metrics
            #     separator: ";"
            #     target_label: endpoint
            #   - action: replace
            #     replacement: "kubelet"
            #     target_label: job
            #   - action: replace
            #     regex: "(.*)"
            #     replacement: "${1}"
            #     separator: ";"
            #     source_labels:
            #       - __meta_kubernetes_node_name
            #     target_label: node
            #   - action: replace
            #     regex: "(.*)"
            #     replacement: "$1"
            #     separator: ";"
            #     source_labels:
            #       - __metrics_path__
            #     target_label: metrics_path

  processors:
    resourcedetection:
      detectors: [env]
      override: true
      timeout: 2s
    k8sattributes:
      auth_type: "serviceAccount"
      passthrough: false
      # filter:
      #   node_from_env_var: K8S_NODE_NAME
      extract:
        labels:
          - tag_name: service.name
            key: app.kubernetes.io/name
            from: pod
          - tag_name: service.name
            key: k8s-app
            from: pod
          - tag_name: k8s.app.instance
            key: app.kubernetes.io/instance
            from: pod
          - tag_name: service.version
            key: app.kubernetes.io/version
            from: pod
          - tag_name: k8s.app.component
            key: app.kubernetes.io/component
            from: pod
        metadata:
          - k8s.namespace.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.node.name
          - k8s.pod.start_time
          - k8s.deployment.name
          - k8s.replicaset.name
          - k8s.replicaset.uid
          - k8s.daemonset.name
          - k8s.daemonset.uid
          - k8s.job.name
          - k8s.job.uid
          - k8s.container.name
          - k8s.cronjob.name
          - k8s.statefulset.name
          - k8s.statefulset.uid
          - container.image.tag
          - container.image.name
          - k8s.cluster.uid
      pod_association:
        - sources:
            - from: resource_attribute
              name: k8s.pod.uid
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
            - from: resource_attribute
              name: k8s.node.name
        - sources:
            - from: resource_attribute
              name: k8s.pod.ip
        - sources:
            - from: resource_attribute
              name: k8s.pod.name
            - from: resource_attribute
              name: k8s.namespace.name
        - sources:
            - from: connection
    # memory_limiter:
    #   check_interval: 1s
    #   limit_percentage: 75
    #   spike_limit_percentage: 15
    batch:
      send_batch_size: 10000
      timeout: 10s
  extensions:
    zpages: {}
    # memory_ballast:
    #   # Memory Ballast size should be max 1/3 to 1/2 of memory. It's a large buffer that is pre-allocated in the memory to help stabilize and reduce the garbage collection (GC) pressure on the Go runtime.
    #   size_mib: 512
  connectors:
    spanmetrics:
      histogram:
        explicit:
          buckets:
            [
              100us,
              1ms,
              2ms,
              6ms,
              10ms,
              100ms,
              250ms,
              500ms,
              1000ms,
              1400ms,
              2000ms,
              5s,
              10s,
              30s,
              60s,
              120s,
              300s,
              600s,
            ]
      dimensions:
        - name: http.method
          default: GET
        - name: http.status_code
      exemplars:
        enabled: true
      dimensions_cache_size: 1000
      aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"
      metrics_flush_interval: 15s
    servicegraph:
      latency_histogram_buckets: [1, 2, 3, 4, 5]
      dimensions:
        - http.method
      store:
        ttl: 1s
        max_items: 10
  service:
    extensions: [zpages]
    pipelines:
      logs/k8s_events:
        receivers: [k8s_events]
        processors: [batch, k8sattributes, resourcedetection]
        exporters: [otlphttp/openobserve_k8s_events]
      metrics:
        receivers: [k8s_cluster, spanmetrics, servicegraph]
        processors: [batch, k8sattributes, resourcedetection]
        exporters: [otlphttp/openobserve]
      traces:
        receivers: [otlp]
        processors: [batch, k8sattributes, resourcedetection]
        exporters: [otlphttp/openobserve, spanmetrics, servicegraph]
