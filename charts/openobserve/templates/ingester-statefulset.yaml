apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "openobserve.fullname" . }}-ingester
  namespace: {{ .Release.Namespace | quote }}
  labels:
    {{- include "openobserve.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.ingester.enabled }}
  replicas: {{ .Values.replicaCount.ingester }}
  {{- end }}
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0  # Set to 0 to allow all pods to update in parallel
  selector:
    matchLabels:
      {{- include "openobserve.selectorLabels" . | nindent 6 }}
      role: ingester
  {{ if .Values.ingester.headless.enabled}}
  serviceName: {{ include "openobserve.fullname" . }}-ingester-headless
  {{- end }}
  {{- if not .Values.ingester.headless.enabled}}
  serviceName: {{ include "openobserve.fullname" . }}
  {{- end }}
  template:
    metadata:
      annotations:
        {{- with .Values.podAnnotations.ingester }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
        checksum/specific-config: {{ include (print $.Template.BasePath "/ingester-configmap.yaml") . | sha256sum }}
        checksum/generic-config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
      labels:
        {{- include "openobserve.selectorLabels" . | nindent 8 }}
        role: ingester
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "openobserve.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      
      initContainers:
        {{- if and .Values.ingester.persistence.volumePermissions.enabled .Values.ingester.persistence.enabled }}
        - name: volume-permissions
          image: {{ .Values.image.busybox.repository }}:{{ .Values.image.busybox.tag }}
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
            - |
              chown -R {{ .Values.podSecurityContext.runAsUser }}:{{ .Values.podSecurityContext.fsGroup }} /data
          securityContext:
            runAsUser: 0
            runAsNonRoot: false
          volumeMounts:
            - name: data
              mountPath: /data
        {{- end }}
        {{- if .Values.etcd.enabled }}
        - name: check-etcd
          image: {{ .Values.image.busybox.repository }}:{{ .Values.image.busybox.tag }}
          command: ['sh', '-c', '
            until nc -z {{ .Release.Name }}-etcd 2379; do
              echo "Waiting for etcd to be ready";
              sleep 5;
            done;
          ']
        {{- end }} 
        {{- if .Values.nats.enabled }}
        - name: check-nats
          image: {{ .Values.image.busybox.repository }}:{{ .Values.image.busybox.tag }}
          resources:
            limits:
              cpu: 50m
              memory: 50Mi
          command: ['sh', '-c', '
            until nc -zv {{ .Release.Name }}-nats 4222; do
              echo "Waiting for NATS to be ready...";
              sleep 2;
            done;
          ']
         {{- end }} 
      containers:
        - name: {{ .Chart.Name }}-ingester
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          {{- if .Values.enterprise.enabled }}
          image: "{{ .Values.image.enterprise.repository }}:{{ .Values.image.enterprise.tag | default .Chart.AppVersion }}"
          {{- else }}
          image: "{{ .Values.image.oss.repository }}:{{ .Values.image.oss.tag | default .Chart.AppVersion }}"
          {{- end }}
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.config.ZO_HTTP_PORT }}
            - name: grpc
              containerPort: {{ .Values.config.ZO_GRPC_PORT }}
          {{- if .Values.probes.ingester.enabled }}
          livenessProbe:
            httpGet:
              path: /healthz
              port: {{ .Values.config.ZO_HTTP_PORT }}
            initialDelaySeconds: {{ .Values.probes.ingester.config.livenessProbe.initialDelaySeconds | default 10 }}
            periodSeconds: {{ .Values.probes.ingester.config.livenessProbe.periodSeconds | default 10 }}
            timeoutSeconds: {{ .Values.probes.ingester.config.livenessProbe.timeoutSeconds | default 10 }}
            successThreshold: {{ .Values.probes.ingester.config.livenessProbe.successThreshold | default 1 }}
            failureThreshold: {{ .Values.probes.ingester.config.livenessProbe.failureThreshold | default 3 }}
            terminationGracePeriodSeconds: {{ .Values.probes.ingester.config.livenessProbe.terminationGracePeriodSeconds | default 30 }}
          readinessProbe:
            httpGet:
              path: /healthz
              port: {{ .Values.config.ZO_HTTP_PORT }}
            initialDelaySeconds: {{ .Values.probes.ingester.config.readinessProbe.initialDelaySeconds | default 10 }}
            periodSeconds: {{ .Values.probes.ingester.config.readinessProbe.periodSeconds | default 10 }}
            timeoutSeconds: {{ .Values.probes.ingester.config.readinessProbe.timeoutSeconds | default 10 }}
            successThreshold: {{ .Values.probes.ingester.config.readinessProbe.successThreshold | default 1 }}
            failureThreshold: {{ .Values.probes.ingester.config.readinessProbe.failureThreshold | default 3 }}
          {{- end }}
          {{- if .Values.autoscaling.ingester.enabled }}
          lifecycle:
            preStop:
              exec:
                command:
                - /bin/sh
                - -c
                - |
                  # Get credentials from environment variables
                  USER_EMAIL="$ZO_ROOT_USER_EMAIL"
                  USER_PASSWORD="$ZO_ROOT_USER_PASSWORD"
                  
                  # Create base64 encoded credentials for Authorization header
                  AUTH_HEADER=$(echo -n "${USER_EMAIL}:${USER_PASSWORD}" | base64)
                  
                  # Disable the node first
                  echo "Disabling ingester node..."
                  curl -X PUT "http://localhost:{{ .Values.config.ZO_HTTP_PORT }}/node/enable?value=false" \
                    -H "Authorization: Basic ${AUTH_HEADER}" 
                  
                  # returns 200 if successful and "true" if the node is disabled

                  # Flush all data from memory to WAL. This does not flush data from ingester to s3.
                  echo "Flushing data from ingester..."
                  RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" -X PUT "http://localhost:{{ .Values.config.ZO_HTTP_PORT }}/node/flush" \
                    -H "Authorization: Basic ${AUTH_HEADER}")
                  if [ "$RESPONSE" -ne 200 ]; then
                    echo "Error: Failed to flush data from ingester. HTTP response code: $RESPONSE"
                    exit 1
                  fi
                  
                  # returns 200 if successful and "true" if the node is flushed

                  # We need another API to check if all the data has been moved to s3 or /flush should become async and move files to s3 as well
                  # e.g /node/wal_status
                  # Need to build this API. Until then, we will wait for 900 seconds.

                  # Wait for 900 seconds after flush to ensure data is moved to s3
                  # 15 minutes for now, since file movement to s3 may take up to 10 minutes
                  echo "Waiting 900 seconds to flush data..."
                  sleep 900
                  
                  echo "Pre-stop hook completed"
          {{- end }}
          resources:
            {{- toYaml .Values.resources.ingester | nindent 12 }}
          envFrom:
            - configMapRef:
                name: {{ include "openobserve.fullname" . }}
            - configMapRef:
                name: {{ include "openobserve.fullname" . }}-ingester
            - secretRef:
                name: {{ if .Values.externalSecret.enabled }}{{ .Values.externalSecret.name }}{{ else }}{{ include "openobserve.fullname" . }}{{ end }}
          env:
            - name: ZO_NODE_ROLE
              value: "ingester"
            {{- with .Values.extraEnv }}
            {{- toYaml . |  nindent 12 }}
            {{- end }}
            {{- with .Values.ingester.extraEnv }}
            {{- toYaml . |  nindent 12 }}
            {{- end }}
          {{- if .Values.ingester.persistence.enabled }}
          volumeMounts:
            - name: data
              mountPath: /data
          {{- end }}
      {{- with .Values.nodeSelector.ingester }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity.ingester }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations.ingester }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
  {{- if .Values.ingester.persistence.enabled }}
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      {{- range .Values.ingester.persistence.accessModes }}
        - {{ . | quote }}
      {{- end }}
      storageClassName: {{ .Values.ingester.persistence.storageClass }}
      resources:
        requests:
          storage: {{ .Values.ingester.persistence.size | quote }}
  {{- end }}

